# for these models, use transformers 4.42.2 (pip install transformers==4.42.2) and peft==0.13.2
# from .video_llama_1.inference import model_init, video_llama_process
# from .PandaGPT.inference import pandagpt_model_init, pandagpt_process
# from .Phi4.inference import phi4_model_init, phi4_process
# from .OneLLM.inference import onellm_model_init, onellm_process

# from .VideoLLaMA2.inference import video_llama2_process, video_llama2_model_init

# from .unified_io_2.inference import uio2_model_init, uio2_process

# from .NExTGPT.inference import NExTGPT_model_init, NExTGPT_process

# from .AnyGPT.inference import anygpt_model_init, anygpt_process

# from .VITA_1.inference import vita1_model_init, vita1_process
# from .VITA_1_5.inference import vita1_5_model_init, vita1_5_process

# from .LAVIS_XInstructBLIP.inference import xblip_model_init, xblip_process
# from .Qwen2_5Omni.inference import qwen2_5Omni_model_init, qwen2_5Omni_process
# from .StreamOmni.inference import streamomni_model_init, streamomni_process
# from .Ola.inference import ola_model_init, ola_process

# from .Qwen3Omni.inference import qwen3omni_model_init, qwen3omni_process